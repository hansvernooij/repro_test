---
title: Modelling TB - risk factors, 
    Taweepoke
author: "Hans Vernooij, 
     Department of Farm Animal Health, Utrecht University, The Netherlands"
date: "Juli 2018"
output: word_document
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

cat("\nRun date:", format(Sys.Date(), "%d %b %Y"))
```


# New approach for analysis of TB - cross sectional data

```{r ReadData, echo=FALSE}
crosssection803_20100813_2016May18 <- read.delim("../GISdata/crosssection803_20100813_2016May18.txt")

# exclude longitudinal samples
attach(crosssection803_20100813_2016May18)
cat("=== Variable names ===")
names(crosssection803_20100813_2016May18)
cat("=== Number of records in initial data:", nrow(crosssection803_20100813_2016May18))
cat("\n === Select only cross sectional elephants with job=2 (tourists) ===")

cross_sectional <- crosssection803_20100813_2016May18[data.cate=="CS"&job==2,]
cat("=== Number of records data after selection:", nrow(cross_sectional))
detach(crosssection803_20100813_2016May18)

#cross_sectional <- cross_sectional[,c(1:20)]
# coding variables as factors
for (i in c(12, 20)) {
  cross_sectional[,i] <- factor(cross_sectional[,i])
}
rm(i)
cross_sectional$AgeCat <- cut(cross_sectional$age.y., c(0, 3, 10, 50, 60, 100))

#
#names(cross_sectional)

cat("\n === Simple summary per variable of crosse sectional data on elephants working in tourist industry ===\n")
summary(cross_sectional)
```

# Demographics of the elephants and summary of tests

```{r summary, echo=FALSE}
attach(cross_sectional)
#cat("\nNumber of sampled elephants from same camp")
freq.of.sampsize <- table(campandnamecate)
#cat("\nFrequency of sample size")
#table(freq.of.samples.per.samp.size=freq.of.sampsize)
#cat("=== 56 camps with 1 elephant and 1 camp with 49 elephants)")

samp.size <- rep(NA, nrow(cross_sectional))
nm <- names(freq.of.sampsize)
for (i in (1:length(freq.of.sampsize))) {
  samp.size[campandnamecate==nm[i]] <- as.numeric(freq.of.sampsize[i]) ##### is het tellen wel goed gegaan?
}
#table(samp.size, campandnamecate)
cross_sectional$samp.size <- samp.size
rm(samp.size, freq.of.sampsize, i, nm)
detach(cross_sectional)

attach(cross_sectional)
cat("\n=== Age y : frequencies and proportions ===")
summary(age.y.)
cat("\nStDev=", sd(age.y.))

cat("\n=== sex : frequencies and proportions ===")
table(sex)
round(prop.table(table(sex)), 2)

cat("\n=== Body condition score : frequencies and proportions ===")
table(BCS)
round(prop.table(table(BCS)), 2)

cat("\n=== Age category : frequencies and proportions  ===")
table(AgeCat)
round(prop.table(table(AgeCat)), 2)

cat("\n=== workperiod h : frequencies and proportions  ===")
table(workperiod.h.)
round(prop.table(table(workperiod.h.)), 2)

cat("\n=== feedtype : frequencies and proportions  ===")
table(feedtype)
round(prop.table(table(feedtype)), 2)

cat("\n=== sample size (number of observations from same camp but total is unknown) : \n    frequencies and proportions  ===")
table(samp.size)
cat("\n   Grouped in sample size categories")
round(prop.table(table(cut(samp.size, c(0, 5, 10, 15, 25, 50)))), 2)

cat("\n=== region : frequencies and proportions  ===")
table(region)
round(prop.table(table(region)), 2)

cat("\n=== province : frequencies and proportions  ===")
table(province)
round(prop.table(table(province)), 2)

cat("\n=== RT-test : frequencies and proportions  ===")
table(RT)
round(prop.table(table(RT)), 2)
binom.test(table(RT)[2], sum(table(RT)))

cat("\n=== EC-test : summary  ===")
summary(EC)
plot(density(EC))

cat("\n=== ESAT6-test : summary  ===")
summary(ESAT6)
plot(density(ESAT6))

cat("\n=== CFP10-test : summary  ===")
summary(CFP10)
plot(density(CFP10))

cat("\n=== MPB83-test : summary  ===")
summary(MPB83)
plot(density(MPB83))

detach(cross_sectional)
```

# Cutoff values for tests

see Shalu Verma-Kumar https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0049548

Cutoff values for
- CFP10 : >= 0.337 (positive)   Is this OD value but not x100 percent?
- ESAT6 : >= 0.2 (positive)     Is this OD value but not x100 percent?

see Boadella 2011/2012
- CFP10/ESAT6 : >= 5 relative light units (positive)
- MPB83 : >= 5 relative light units (positive)
- when both test < 5 than negative

```{r cutoff, echo=FALSE}
attach(cross_sectional)
cat("\n=== cutoff value for ESAT6 : 20")
table(ESAT6.positive=ESAT6 >= 20)
round(prop.table(table(ESAT6.positive=ESAT6 >= 20)), 3)
plot(density(ESAT6))
abline(v= 20, lty= 2)

cat("\n=== cutoff value for CFP10 : 33.7")
table(CFP10.positive=CFP10 >= 33.7)
round(prop.table(table(CFP10.positive=CFP10 >= 33.7)), 3)
plot(density(CFP10))
abline(v= 33.7, lty= 2)
detach(cross_sectional)
```

# correlation between tests

```{r correlation, echo=FALSE}
attach(cross_sectional)
round(cor(cross_sectional[,c(13:17)]), 3)
pairs(cross_sectional[,c(13:17)])
detach(cross_sectional)
```


# kmeans for 5 tests: clustering the data into some groups

In the multidimensional space (5 tests) are used to seperate the individuals into clusters.

We have to specify the number of clusters ourselves but we have some possibilities to find the optimum mathematically.

In the kmeans method the test values are standardized to the z-scale.

=== Question is if the RT score (0/1) should be stabdardized too.=====

In the results below WITHOUT standardization of RT 2 clusters are recomended.

WITH standardization of RT 3 clusters are recommended.

For extra information about the methods:

https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a

https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/

https://www.datanovia.com/en/courses/partitional-clustering-in-r-the-essentials/

```{r kmeans, echo=FALSE}
# https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a
# https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/
# https://www.datanovia.com/en/courses/partitional-clustering-in-r-the-essentials/

#pkgs <- c("factoextra",  "NbClust")
#install.packages(pkgs)

library(factoextra)
library(NbClust)

x <- cross_sectional[,c(13:17)] # de kolommen uit de dataset met testen
df <- cbind(RT=x[,1], scale(x[,2:5])) # No standardization of RT
# head(df)

# Elbow method
cat("=== Compute the number of clusters with Elbow method ===")
fviz_nbclust(df, kmeans, method = "wss") +
    geom_vline(xintercept = 3, linetype = 2)+
  labs(subtitle = "Elbow method")
cat("\n=== OR should the best number be 2 clusters?!?!?")

# Silhouette method
cat("=== Compute the number of clusters with Silhouette method ===")
fviz_nbclust(df, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

# Gap statistic
# nboot = 50 to keep the function speedy. 
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
#set.seed(123) # this doesnot converge
#fviz_nbclust(df, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
#  labs(subtitle = "Gap statistic method")

library(cluster)
set.seed(123)
# Compute gap statistic for kmeans
# we used B = 10 for demo. Recommended value is ~500
cat("=== Compute the number of clusters based on the Gap-Statistic ===")
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,
 K.max = 10, B = 10)
 print(gap_stat, method = "firstmax")
fviz_gap_stat(gap_stat)
rm(gap_stat)
cat("==== ==== ==== ====")
cat("\nThe disadvantage of elbow and average silhouette methods is that, \nthey measure a global clustering characteristic only. \nA more sophisticated method is to use the gap statistic which \nprovides a statistical procedure to formalize the elbow/silhouette \nheuristic in order to estimate the optimal number of clusters.")

(result <- NbClust(data = df, diss = NULL, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans"))
#result$All.index
#result$All.CriticalValues
#result$Best.nc
rm(result)

cat("\n\n=== === === Clustering WITHOUT standardization of RT === === === ")
#ratio <- vector()
for (i in (2:4)) { 
  cat("\n=== === Clustering the 5 tests into", i," categories === ===")
  cl<-kmeans(df, i)
  cat("\nCenter means in clusters: \n") 
  print(cbind(round(cl$centers, 4), cluster.size=cl$size))
  #cl$withinss # per group
  #cl$tot.withinss 
  #cl$betweenss
  #cat("\n=== Ratio MSbetween / MSwithin : ", 
    #cl$betweenss/length(cl$withinss) / (cl$tot.withinss/sum(cl$size)))
  #ratio <- c(ratio, cl$betweenss/length(cl$withinss) / (cl$tot.withinss/sum(cl$size)))
  plot(x, col = cl$cluster)
  rm(cl)
}

cat("\n    In previous 3 clustering analysis it seems that ESAT6 is doing different compared with the other 4 tests, see 5x5 graphs\n\n")

cat("\n=== Clusterplot based on kmeans method with 3 clusters")
km.res <- kmeans(df, 3, nstart = 25)
fviz_cluster(km.res, data=df, main= "Cluster plot (original RT)")
rm(km.res)
cat("\n    This is a very weird separation in 3 clusters\n\n")

cat("\n\n=== === === Clustering WITH standardization of RT === === === ")

x <- cross_sectional[,c(13:17)] # de kolommen uit de dataset met testen
df <- cbind(scale(x))

for (i in (2:4)) { 
  cat("\n=== === Clustering the 5 tests into", i," categories === ===")
  cl<-kmeans(df, i)
  cat("\nCenter means in clusters: \n") 
  print(cbind(round(cl$centers, 4), cluster.size=cl$size))
  plot(x, col = cl$cluster)
  rm(cl)
}
rm(i)

cat("\n   When more than 2 clusters then one cluster consists the RT-positive samples")

cat("\n=== Clusterplot based on kmeans method with 3 clusters")
set.seed(123)
km.res <- kmeans(df, 3, nstart = 25)

# Visualize
fviz_cluster(km.res, data=df, main = "Cluster plot (standardized RT)")
#table(cross_sectional$RT, km.res$cluster)
cat("\n    The points in the upper cluster are all RT-positive samples\n\n")

# Tryout to use kmeans on continuous test results
#x <- cross_sectional[,c(14:17)] # The columns with test results but not RT
#df <- scale(x)
#set.seed(123)
#km.res <- kmeans(df, 2, nstart = 25)

# Visualize
#fviz_cluster(km.res, data=df, main = "Cluster plot (standardized RT)")
#table(cross_sectional$RT, km.res$cluster)
```

# Latent Class analysis

... relates a set of observed (usually discrete) multivariate variables to a set of latent variables

see https://en.wikipedia.org/wiki/Latent_class_model

see https://cran.r-project.org/web/packages/sBIC/vignettes/LatentClassAnalysis.pdf


```{r LCA, echo=FALSE}
# ways to do Latent class analysis https://maksimrudnev.com/2016/12/28/latent-class-analysis-in-r/
# see for stepwise explanation https://statistics.ohlsen-web.de/latent-class-analysis-polca/
install.packages("poLCA")
library("poLCA")
attach(cross_sectional)
EC.pos <- ifelse(EC >= 20, 2, 1) ## just guess
ESAT6.pos <- ifelse(ESAT6 >= 20, 2, 1)
CFP10.pos <- ifelse(CFP10 >= 33.7, 2, 1)
MPB83.pos <- ifelse(MPB83 >= 33.7, 2, 1) ## just guess
RT.pos <- RT+1

testdata <- data.frame(EC.pos, ESAT6.pos, CFP10.pos, MPB83.pos, RT.pos)
rm(EC.pos, ESAT6.pos, CFP10.pos, MPB83.pos, RT.pos)
summary(testdata)
detach(cross_sectional)

# select variables
#mydata <- data %>% dplyr::select(F29_a,F29_b,F29_c,F27_a,F27_b,F27_e,F09_a, F09_b, F09_c)

# define function
#f<-with(mydata, cbind(F29_a,F29_b,F29_c,F27_a,F27_b,F27_e,F09_a, F09_b, F09_c)~1) #
f <- with(testdata, cbind(EC.pos, ESAT6.pos, CFP10.pos, MPB83.pos, RT.pos) ~ 1)

#------ run a sequence of models with 1-10 classes and print out the model with the lowest BIC
max_II <- -100000
min_bic <- 100000
for(i in 2:5){
  lc <- poLCA(f, testdata, nclass=i, maxiter=3000, 
              tol=1e-5, na.rm=FALSE,  
              nrep=10, verbose=TRUE, calc.se=TRUE)
  if(lc$bic < min_bic){
    min_bic <- lc$bic
    LCA_best_model<-lc
  }
}    	
LCA_best_model
# str(LCA_best_model)
cat("=== Predicted class by LCA (1= negative, 2= positive)")
table(predicted.class=LCA_best_model $ predclass)
round(prop.table(table(predicted.class=LCA_best_model $ predclass)), 3)

attach(testdata)
table(LCA_best_model $ predclass, ESAT6.pos)
round(prop.table(table(pred.class=LCA_best_model $ predclass, ESAT6.pos), 1), 3)

table(LCA_best_model $ predclass, CFP10.pos)
round(prop.table(table(pred.class=LCA_best_model $ predclass, CFP10.pos), 1), 3)

table(LCA_best_model $ predclass, RT.pos)
round(prop.table(table(pred.class=LCA_best_model $ predclass, RT.pos), 1), 3)

set.seed(234567)
lc.x <- poLCA(f, testdata, nclass=2, maxiter=3000, 
          tol=1e-5, na.rm=FALSE, nrep=10, verbose=TRUE, calc.se=TRUE, graphs = T)
legend(1.2, 5.2, legend="Pr(True positive)", col="black", bty="n")
legend(1.9, 5.2, legend="Pr(False positive)", col="black", bty="n")
cat("=== Plot shows on right axis: proportion positive (TP) based on test given predicted POSITIVE class")
cat("=== Plot shows on left axis: proportion positive (FP) based on test given predicted NEGATIVE class")
detach(testdata)

```

# LCA with Dependent Mixture Models

see https://maksimrudnev.com/2016/12/28/latent-class-analysis-in-r/#depmixS4

```{r PCA,echo=FALSE}
install.packages("depmixS4")
library(depmixS4)
# ALL 5 tests
model_definition <- mix(list(EC ~ 1, ESAT6 ~ 1, CFP10 ~ 1, MPB83 ~ 1, RT ~1), family = list(gaussian(), #For every corresponding 
 gaussian(),  #  indicator a family of distribution 
 gaussian(),  #  indicator a family of distribution 
 gaussian(),  #  indicator a family of distribution 
 multinomial("identity")), # should be indicated in the list.
 data = cross_sectional,
 nstates = 2, #This is the number of classes
 nstart=c(0.9, 0.1), # Prior probabilities of classes
 initdata = cross_sectional #Our data
)
set.seed(123456)
fit.mod <- fit(model_definition)   # Fit the model

fit.mod
summary(fit.mod)
#str(fit.mod)

# Without RT
model_definition <- mix(list(EC ~ 1, ESAT6 ~ 1, CFP10 ~ 1, MPB83 ~ 1), family = list(gaussian(), #For every corresponding 
 gaussian(),  #  indicator a family of distribution 
 gaussian(),  #  indicator a family of distribution 
 gaussian()), # should be indicated in the list.
 data = cross_sectional,
 nstates = 2, #This is the number of classes
 nstart=c(0.9, 0.1), # Prior probabilities of classes
 initdata = cross_sectional #Our data
)
set.seed(123456)
fit.mod <- fit(model_definition)   # Fit the model

fit.mod
summary(fit.mod)
#str(fit.mod)

# ALL 5 tests, but 3 classes
model_definition <- mix(list(EC ~ 1, ESAT6 ~ 1, CFP10 ~ 1, MPB83 ~ 1, RT ~1), family = list(gaussian(), #For every corresponding 
 gaussian(),  #  indicator a family of distribution 
 gaussian(),  #  indicator a family of distribution 
 gaussian(),  #  indicator a family of distribution 
 multinomial("identity")), # should be indicated in the list.
 data = cross_sectional,
 nstates = 3, #This is the number of classes
 nstart=c(0.9, 0.1), # Prior probabilities of classes
 initdata = cross_sectional #Our data
)
set.seed(123456)
fit.mod <- fit(model_definition)   # Fit the model

fit.mod
summary(fit.mod)
#str(fit.mod)

# WITHOUT test RT, but 3 posterior classes
model_definition <- mix(list(EC ~ 1, ESAT6 ~ 1, CFP10 ~ 1, MPB83 ~ 1, RT ~1), family = list(gaussian(), #For every corresponding 
 gaussian(),  #  indicator a family of distribution 
 gaussian(),  #  indicator a family of distribution 
 gaussian(),  #  indicator a family of distribution 
 multinomial("identity")), # should be indicated in the list.
 data = cross_sectional,
 nstates = 3, #This is the number of classes
 nstart=c(0.9, 0.1), # Prior probabilities of classes
 initdata = cross_sectional #Our data
)
set.seed(123456)
fit.mod <- fit(model_definition)   # Fit the model

fit.mod
summary(fit.mod)
#str(fit.mod)
```

# Principal component analysis on standardized test results

This method is trying to capture as much variation in the test-data and to assign that to different components. The new components are orthogonal and can indicate underlying groups.

It is quite similar to the kmeans method.

Finally this method is not very usefull.

```{r PCA,echo=FALSE}
cat("\n === === PCA when RT is standardized === ===")
x <- cross_sectional[,c(13:17)] # de kolommen uit de dataset met testen
df <- scale(x)
fit <- princomp(df) # Because the variables are scaled, no need to set cor=TRUE
summary(fit)
#plot(fit)
biplot(fit)
fit$loadings[,1:5]
rm(fit)
cat("\n   the seperated points are the RT-positive samples")

cat("\n === === PCA when RT is not standardized === ===")
x <- cross_sectional[,c(13:17)] # de kolommen uit de dataset met testen
df <- cbind(RT=x[,1], scale(x)[, 2:5]) 
# Because the variables are scaled, no need to set cor=TRUE; otherwise RT will be scaled too!
fit <- princomp(df)
summary(fit)
#plot(fit)
biplot(fit)
fit$loadings[,1:5]
rm(fit)

```

```{r SessionInfo,echo=FALSE}
sessionInfo()
citation()
citation("lme4")
citation("factoextra")
citation("NbClust")
```

\end{document}